{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# papeles package - parsing files examples\n",
    "\n",
    "\n",
    "In this notebook, there are 3 examples of things you might want to do with the raw PDF files you might want to do analysis on. Using the data extracted from the [neurips_crawler](https://github.com/glhuilli/neurips_crawler), raw PDF files are processed, and two sub tasks are presented: extract paper sentences (everything that is not references or header) and extract header (everything above the abstract). \n",
    "\n",
    "Some applications like references extraction will be added in future applications. \n",
    "\n",
    "\n",
    "## Process PDF files \n",
    "\n",
    "using `neurips.load.load_folder` you'll be able to process Neurips PDF files and extract as a generator that can be used for processing purposes (e.g. the ones presented in the following sections). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8c9286b2334213beeec7853814f78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "files: 5327\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from papeles.corpus.neurips.load import load_folder\n",
    "\n",
    "# output is the output folder from neurips_crawler: https://github.com/glhuilli/neurips_crawler\n",
    "NEURIPS_DATA_OUTPUT = '/var/data/neurips_crawler/output'  \n",
    "\n",
    "\n",
    "metadata = {}  # year -> NeurIPS object \n",
    "files_data = {}  # file_name -> text of file\n",
    "for folder in tqdm(os.listdir(NEURIPS_DATA_OUTPUT)):\n",
    "    year = folder.split('_')[-1]\n",
    "    files_sentences, files_metadata = load_folder(os.path.join(NEURIPS_DATA_OUTPUT, folder))\n",
    "    metadata[year] = files_metadata\n",
    "    files_data.update(files_sentences)\n",
    "print(f'files: {len(files_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'a9f07f6e-073c-5dca-9071-256ffc6dec5b',\n",
       " 'title': 'Information-theoretic lower bounds on the oracle complexity of convex optimization',\n",
       " 'pdf_name': '3689-information-theoretic-lower-bounds-on-the-oracle-complexity-of-convex-optimization.pdf',\n",
       " 'abstract': 'Despite the large amount of literature on upper bounds on complexity of convex analysis, surprisingly little is known about the fundamental hardness of these problems. The extensive use of convex optimization in machine learning and statistics makes such an understanding critical to understand fundamental computational limits of learning and estimation. In this paper, we study the complexity of stochastic convex optimization in an oracle model of computation. We improve upon known results and obtain tight minimax complexity estimates for some function classes. We also discuss implications of these results to the understanding the inherent complexity of large-scale learning and estimation problems.',\n",
       " 'authors': [{'id': 'alekh-agarwal-3606', 'name': 'Alekh Agarwal'},\n",
       "  {'id': 'martin-j-wainwright-1952', 'name': 'Martin J. Wainwright'},\n",
       "  {'id': 'peter-l-bartlett-646', 'name': 'Peter L. Bartlett'},\n",
       "  {'id': 'pradeep-k-ravikumar-3253', 'name': 'Pradeep K. Ravikumar'}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['2009']['3689-information-theoretic-lower-bounds-on-the-oracle-complexity-of-convex-optimization.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3689-information-theoretic-lower-bounds-on-the-oracle-complexity-of-convex-optimization.pdf'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(files_data.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract \"Sentences\" of Papers\n",
    "\n",
    "Each paper has different sections that might be of relevance for different applications. In this case, `paper.get_sentences` is mainly focused in getting all those lines in the paper that are related to the main content of the paper (before the \"references\" and after the \"abstract\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from papeles.utils import paper\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "NEURIPS_ANALYSIS_DATA = '/var/data/neurips_analysis/'\n",
    "\n",
    "\n",
    "# Note that this takes ~2hrs to run. \n",
    "for paper_key, paper_sentences in tqdm(list(files_data.items())):\n",
    "    with open(os.path.join(NEURIPS_ANALYSIS_DATA, f'files_sentences/{paper_key}_sentences.txt'), 'w') as f:\n",
    "        for line in paper.get_sentences(paper.flatten(paper_sentences)):\n",
    "            f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract \"Header\" of Papers \n",
    "\n",
    "To get the header of a paper, which is defined as everything right before the abstract (i.e. title, authors, etc.), you can use the `paper.get_header` method as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f105e25be5249efb95ec415f266e8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5327.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for paper_key, paper_sentences in tqdm(list(files_data.items())):\n",
    "    with open(os.path.join(NEURIPS_ANALYSIS_DATA, f'files_headers/{paper_key}_headers.txt'), 'w') as f:\n",
    "        for line in paper.get_header(paper.flatten(paper_sentences)):\n",
    "            f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save paper metadata\n",
    "\n",
    "In the particular case of Neurips, there's metadata available, which is recommended to be stored independently from other files, and it shows information as the year of publication, authors (author_id and author_name), and the paper_key (number at the beginning of the file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953f08c797e7450b878922bf3b9066af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from papeles.paper.neurips import get_key\n",
    "\n",
    "\n",
    "for year, paper_data in tqdm(data.items()):\n",
    "    for paper_key, file_metadata in paper_data.items():\n",
    "        file_metadata = metadata[year][paper_key]\n",
    "        file_metadata['year'] = int(year)\n",
    "        file_metadata['paper_key'] = get_key(paper_key)\n",
    "        with open(os.path.join(NEURIPS_ANALYSIS_DATA, f'files_metadata/{paper_key}_metadata.json'), 'w') as f:\n",
    "            json.dump(file_metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
